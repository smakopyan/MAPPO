# MAPPO: Многоагентная оптимизация политики на основе проксимальных методов для TurtleBot3

## Обзор

MAPPO (Multi-Agent Proximal Policy Optimization) — это проект машинного обучения с подкреплением, предназначенный для обучения нескольких роботов TurtleBot3 автономной навигации и планированию пути с использованием алгоритма PPO. Проект интегрирует глубокое обучение с подкреплением с алгоритмом планирования пути Theta*, обеспечивая эффективную координацию многоагентных систем в симулированной и реальной среде.

## Основные возможности

- **Многоагентное обучение PPO**: реализация алгоритма Proximal Policy Optimization для одновременного обучения нескольких роботов
- **Планирование пути Theta***: интеграция продвинутого алгоритма планирования пути Theta* для оптимальной маршрутизации
- **Поддержка TurtleBot3**: совместимость с моделями TurtleBot3 Burger и Waffle
- **Интеграция с Gazebo**: полная поддержка симуляции с предварительно настроенными окружениями
- **Возможность SLAM**: поддержка SLAM (одновременная локализация и картирование) для развертывания в реальных условиях
- **Развертывание на реальных роботах**: файлы запуска для симулированных и реальных роботов
- **Визуализация в RViz**: предварительно настроенные макеты визуализации для одного и нескольких роботов
- **Множество тестовых окружений**: разнообразные миры и карты (лабиринты, тестовые сценарии, стандартные окружения)

## Структура проекта

```
MAPPO/
├── src/                           # Исходный код
│   ├── actor.py                   # Реализация актора (политики)
│   ├── critic.py                  # Реализация критика (функция ценности)
│   ├── train.py                   # Основной цикл обучения PPO
│   ├── test.py                    # Скрипты тестирования и оценки
│   ├── theta_star.py              # Алгоритм планирования пути Theta*
│   ├── turtlebot_env.py           # Пользовательское окружение Gym для TurtleBot3
│   ├── ppo_actor.keras            # Предтренированная модель актора
│   └── ppo_actor_best.keras       # Лучшая обученная модель актора
├── launch/                        # Файлы запуска ROS 2
│   ├── simulation_basic.launch.py         # Базовая симуляция
│   ├── simulation_slam.launch.py          # Симуляция с SLAM
│   ├── simulation_only_world.launch.py    # Только мир
│   └── real_world.launch.py               # Развертывание на реальных роботах
├── worlds/                        # Миры Gazebo
│   ├── maze.sdf                   # Лабиринт
│   ├── test.sdf, test1.sdf        # Тестовые сценарии
│   ├── world1.sdf, world2.sdf, world3.sdf
│   └── turtlebot3_world.sdf
├── maps/                          # Предгенерированные карты
│   ├── maze.yaml, maze.pgm
│   ├── map1.yaml, map1.pgm
│   ├── map2.yaml, map2.pgm
│   ├── map3.yaml, map3.pgm
│   └── test*.yaml, test*.pgm
├── urdf/                          # Описания роботов
│   ├── turtlebot3_burger.urdf
│   └── turtlebot3_waffle.urdf
├── rviz/                          # Конфигурационные файлы RViz
│   ├── single_robot_view.rviz         # Для одного робота
│   ├── multi_robot_view.rviz          # Для нескольких роботов
│   └── multi_robot_view_train.rviz    # Для обучения
├── models/                        # Дополнительные файлы моделей
├── test/                          # Тесты
└── package.xml                    # Метаданные пакета ROS 2
```

## Установка

### Требования

- **ОС**: Linux (рекомендуется Ubuntu 20.04 или 22.04)
- **ROS 2**: Humble или новее
- **Python**: 3.8+
- **TensorFlow/Keras**: 2.13+
- **Зависимости**: NumPy, SciPy, OpenCV

### Пошаговая установка

1. **Клонирование репозитория** (если еще не в рабочей области):
   ```bash
   cd ~/turtlebot3_ws/src
   git clone <url-репозитория> MAPPO
   ```

2. **Установка зависимостей**:
   ```bash
   cd ~/turtlebot3_ws
   rosdep install --from-paths src --ignore-src -r -y
   pip install tensorflow keras numpy scipy opencv-python matplotlib pyyaml gym rclpy
   ```

3. **Сборка рабочей области**:
   ```bash
   cd ~/turtlebot3_ws
   colcon build --symlink-install
   ```

4. **Загрузка переменных окружения**:
   ```bash
   source ~/turtlebot3_ws/install/setup.bash
   ```

## Использование

### Обучение модели

Для обучения агента PPO с нуля:

```bash
cd ~/turtlebot3_ws/src/MAPPO/src
python3 train.py
```

**Основные параметры конфигурации в `train.py`**:
- `gamma`: коэффициент дисконтирования (по умолчанию: 0.99)
- `epsilon`: параметр отсечения PPO (по умолчанию: 0.1)
- `actor_lr`: скорость обучения актора (по умолчанию: 0.0003)
- `critic_lr`: скорость обучения критика (по умолчанию: 0.0003)
- `gaelam`: параметр лямбда GAE (по умолчанию: 0.95)

### Симуляция - Базовая версия

Запуск симуляции с несколькими роботами в предопределенном мире:

```bash
ros2 launch theta_star simulation_basic.launch.py
```

### Симуляция - С SLAM

Запуск симуляции с включенным картированием SLAM:

```bash
ros2 launch theta_star simulation_slam.launch.py
```

### Симуляция - Только мир

Запуск только окружения Gazebo без роботов:

```bash
ros2 launch theta_star simulation_only_world.launch.py
```

### Развертывание на реальных роботах

Запуск обученных агентов на реальных роботах TurtleBot3:

```bash
ros2 launch theta_star real_world.launch.py
```

### Тестирование

Оценка обученной модели:

```bash
cd ~/turtlebot3_ws/src/MAPPO/src
python3 test.py
```

### Визуализация в RViz

Открытие RViz с предварительно настроенными конфигурациями:

```bash
# Для одного робота
ros2 run rviz2 rviz2 -d ~/turtlebot3_ws/src/MAPPO/rviz/single_robot_view.rviz

# Для нескольких роботов
ros2 run rviz2 rviz2 -d ~/turtlebot3_ws/src/MAPPO/rviz/multi_robot_view.rviz

# Для визуализации во время обучения
ros2 run rviz2 rviz2 -d ~/turtlebot3_ws/src/MAPPO/rviz/multi_robot_view_train.rviz
```

## Архитектура системы

### Сетевая часть актора (`actor.py`)
- Нейронная сеть, обучающая политику управления
- Выходные данные: среднее значение действия и стандартное отклонение
- Использует активацию tanh для непрерывного управления

### Сетевая часть критика (`critic.py`)
- Аппроксиматор функции ценности состояния
- Предсказывает ценность состояния для расчета преимущества
- Учитывает информацию о карте для лучшего представления состояния

### Окружение (`turtlebot_env.py`)
- Пользовательское окружение Gym для TurtleBot3
- Управляет:
  - Управлением роботом и одометрией
  - Обнаружением препятствий через LaserScan
  - Расчетом системы вознаграждения
  - Условиями достижения цели
  - Агрегацией состояния для нескольких роботов

### Планирование пути (`theta_star.py`)
- Реализация алгоритма Theta* для оптимального планирования
- Генерирует опорные траектории
- Поддерживает динамическое избежание препятствий

## Доступные окружения

| Окружение | Файл | Описание |
|-----------|------|---------|
| Лабиринт | `worlds/maze.sdf` | Сложная навигация в лабиринте |
| Тест 1 | `worlds/test.sdf` | Простой курс с препятствиями |
| Тест расширенный | `worlds/test1.sdf` | Расширенный тестовый сценарий |
| Стандартный мир | `worlds/turtlebot3_world.sdf` | Стандартное окружение TurtleBot3 |

## Мониторинг обучения

Журналы обучения и TensorBoard сводки сохраняются в:
```
logs/ppo/<timestamp>/robot_<id>/
```

Просмотр прогресса обучения:
```bash
tensorboard --logdir=logs/ppo/
```

## Файлы моделей

- `ppo_actor.keras`: последняя обученная модель актора
- `ppo_actor_best.keras`: лучшая обученная модель (на основе валидации)

Загрузка предтренированной модели:
```python
from actor import ImprovedActor
import tensorflow as tf
actor = ImprovedActor(state_dim, action_dim)
actor.load_weights('ppo_actor_best.keras')
```

## Конфигурация

### Параметры робота

Отредактируйте `src/turtlebot_env.py` для изменения:
- `num_robots`: количество роботов в симуляции
- `episode_steps`: максимальное количество шагов за эпизод
- `observation_space`: размерность входных сенсоров
- `action_space`: размерность управляющих действий

### Параметры обучения

Отредактируйте `src/train.py` для изменения:
- Коэффициента дисконтирования (`gamma`)
- Скоростей обучения (`actor_lr`, `critic_lr`)
- Диапазона отсечения PPO (`epsilon`)
- Количества эпизодов обучения
- Размера батча для обновления


## Метрики производительности

Процесс обучения оценивает:
- Среднее вознаграждение за эпизод
- Процент успешных достижений цели
- Среднюю длину пути
- Коэффициент столкновений
- Время обучения за эпизод

## Возможные улучшения

- [ ] Слой коммуникации между агентами
- [ ] Централизованное обучение с децентрализованным исполнением (CTDE)
- [ ] Curriculum learning для сложных окружений
- [ ] Трансферное обучение между разными морфологиями роботов
- [ ] Оптимизация производительности в реальном времени

## Контрибьютинг

Приветствуются вклады! Пожалуйста, убедитесь:
1. Код соответствует стилю PEP 8
2. Все тесты проходят: `pytest test/`
3. Для всех функций добавлены docstring'и
4. Сообщения коммитов информативны

## Лицензия

Apache License 2.0 
